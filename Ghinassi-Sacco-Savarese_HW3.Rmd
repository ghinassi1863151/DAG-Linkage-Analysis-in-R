---
title: "HW3-SDS"
output: html_document
---
## 2. Statistics for universal test 

We implement two universal statistics based on a random split of the data into a train and test set of equal size $n$. The first one is Split Likelihood Ratio:
\begin{align*}
U_n = \dfrac {  \mathcal{L}(\hat{\theta}^\color{red}{Te}| D_n^\color{blue}{Tr})}{   \mathcal{L}(\hat{\theta_0}^\color{blue}{Tr}| D_n^\color{blue}{Tr})  }
\end{align*}

The second one is Cross-Fit Likelihood Ratio:
\begin{align*}
W_n = \dfrac { U_n + U_n^{swap}}{   2  }
\end{align*}

where  $U_n^{swap}$ is the same as $Un$ after swapping the roles of $D_n^\color{blue}{Tr}$ and $D_n^\color{red}{Te}$.
We use the following functions to generate the adjacency matrix $A$ and the corresponding $X$ data (using the linear structural equation model $(3)$ of the paper).


```{r,eval=FALSE}

generate_matrix <- function(p){
  
  # this function generates a random adjacency
  # matrix given a sparsity parameter
  
  sparsity = 2/p
  A <- matrix(rbinom(p*p, 1, sparsity)*sign(runif(p*p, -1, 1))*runif(p*p, 0.7, 1), p, p)
  A[upper.tri(A, diag=TRUE)] <- 0
  return(A)
  
}


generate_data <- function(A, n, p){
  
  # given an adjacency matrix this function generates
  # a data matrix with a gaussian stochastic error
  
  Sigma = solve(diag(p) - A)
  X <- matrix(rnorm(n*p), n, p) %*% t(Sigma)
  return(X)
  
}
```

Then  we split the data by sampling indices randomly (we use two sets of the same size).
```{r, eval=FALSE}
split_data <- function(X, n){
  
  # This function splits the data in train and test set
  # given a split proportion
  
  idx <- sample(1:n,n/2)
  train <- X[idx, ]
  test <- X[-idx,]
  return(list(train=train, test=test))
```
Finally we compute $U_n$ using MLEdag to get the two different estimates of matrix A (under H0 with the train and H1 with the test).

```{r, eval=FALSE}
# Test statistics ---------------------------------------------------------

Un <- function(tr, te, idx, mu, D){
  
  # estimate the Adjacency matrix
  out_tr <- MLEdag(X=tr,D=D,tau=0.35,mu=mu,rho=1.2,trace_obj=FALSE)
  out_te <- MLEdag(X=te,D=D,tau=0.35,mu=mu,rho=1.2,trace_obj=FALSE)
  A.0 = out_tr$A.H0; A = out_te$A.H1
  # estimate sigma for constrained and unconstrained
  sig_alt = sigma_est(A, te, idx)
  sig_null = sigma_est(A.0, tr, idx)
  # compute the test statistic
  x = lik_est(A, tr, idx, sig_alt) - lik_est(A.0, tr, idx, sig_null)
  return(x)
  
}

```
How you can see we used  other "accessory" functions to obtain the estimate of the variance of the errors (sigma) and to calculate the log-likelyhoods (with different estimates of the parameters but always using the train set).
For sigma we use the formula in the originale paper "Likelihood Ratio Tests for a Large Directed Acyclic Graph".
\begin{align*}
\hat{\sigma}^{2}= (np)^{-1} \sum_{j=1}^{p}\sum_{i=1}^{n}(x_{ij}- \sum_{k\neq j}x_{ik}A_{jk})^2
\end{align*}
```{r, eval= FALSE}
sigma_est <-function(A, X, idx){
  
  # this function estimates the sigma of a given matrix
  
  c = 1/(dim(X)[1]*length(idx))
  sss = 0
  for(j in 1:length(idx)) sss = sss + sigma_square(A, X, j, idx)
  return(c*sss)
  
}

sigma_sum <- function(A, X, i, j, idx){
  
  # this function computes the internal sum of the sigma estimate formula
  
  s = 0
  for(k in idx[idx != j]) s = s + X[i, k]*A[j, k]
  return(s)
  
}


sigma_square <- function(A, X, j, idx){
  
  # this function computes the sum of the squared terms in the
  # estimated sigma formula
  
  ss = 0
  for(i in 1:dim(X)[1]) ss = ss + (X[i, j] - sigma_sum(A, X, i, j, idx))**2
  return(ss)
  
}

lik_est <- function(A, X, idx, sig){
  
  # this function estimates the logLikelihood
  # of an adjacency matrix given the data
  
  c = 1/(2*sig)
  a = (dim(X)[1]/2)*log(sig)
  s = 0
  for(j in 1:length(idx)) s = s + c*sigma_square(A, X, j, idx) + a
  return(-s)
  
}


```
We calculate $W_n$ directly when we simulate the test because we simply get it in this way:
(we use exponential for convenience since we work with log-likelyhoods for $U_n$ )
```{r, eval=FALSE}
# compute the test statistics U_n and W_n
  Un_base = Un(tr, te, idx, mu, D)
  Un_swap = Un(te, tr, idx, mu, D)
  Wn = (exp(Un_base) + exp(Un_swap))/2
```
## 3. Size and power of universal tests
The size of a test is defined as

$\alpha$ $= \mathbb{P}$(Commit a type I error) $= \mathbb{P}$(The test says the data and $H0$ are not compatible although $H0$ is true) 
 
while the power is $1- \beta$  where  
$\beta$ $= \mathbb{P}$(Commit a type II error)= $\mathbb{P}$(The test says the data and $H0$ are  compatible although $H0$ is false)

Specifically we implement two different tests for DAG. 


**Graph linkages**
Let $F$ be an index set where an index $(j, k) \in F$ represents a directed connection. We are interested in testing:

$H0$ : $A[j, k] = 0$  $\forall (j, k) \in F$  vs $H1$ : not $H0$ 

To test the power and power of our test, we decided to use the examples that we also find in the papers in order to have a comparison. 
In the first case we are dealing with a random graph and a single linkage is tested.
The code to obtain the power is the one commented, in this case it is enough to change a single arc to have data compatible with H1 and we change the value of our parameters to better understand the quality of our test (as you can see in the table below).
```{r, eval=FALSE}
D = matrix(0, p, p)
D[2,1] = 1
## generate a random lower triangular adjacency matrix
A <- matrix(rbinom(p*p, 1, sparsity)*sign(runif(p*p, -1, 1)), p, p)
A[upper.tri(A, diag=TRUE)] <- 0
A[2,1] = 0 #compatible with H0 for size
#A[2,1]=1 compatible with H1 for power
M = 1000
Unn = rep(NA, M)
Wnn = rep(NA, M)
for(m in 1:M){
  X <- matrix(rnorm(n*p), n, p) %*% t(solve(diag(p) - A))
  # split the data
  
 
  Unn[m] = linkage_test(X, D, idx, alpha,mu)$lrt
  Wnn[m]=linkage_test(X, D, idx, alpha,mu)$crossfit
  
  progress(m, M)
}



cat('\nLRT size: ',sum(Unn)/M , '\n')
cat('Crossfit size: ',sum(Wnn)/M, '\n')
#cat('\nLRT power: ', 1 - (M-sum(Unn))/M, '\n')
#cat('Crossfit power: ', 1 - (M-sum(Wnn))/M, '\n')


```
OSS: we always set p=10 for computational reason


| Parameters                | SIZE                   | POWER                  |
|:-------------------------:|:----------------------:|:----------------------:|
| M=1000, n=200,alpha=0.05  | LRT= 0.097 Cross= 0.015| LRT=0.89 Cross=0.995   |
| M=2000, n=200, alpha=0.05 | LRT= 0.1025 Cross= 0.19| LRT=0.975 Cross=1      |
| M=1000,n=300, alpha=0.05  | LRT= 0.001 Cross= 0.001| LRT=0.955 Cross=1      |
| M=1000,n=500, alpha=0.05  | LRT= 0.0 Cross= 0.0    | LRT=0.992 Cross=1      |
| M=1000,n=200, alpha=0.01  | LRT= 0.064 Cross=0.145 | LRT=0.961  Cross= 1    |
|M=1000, n=1000, alpha=0.05 | LRT= 0.0 Cross= 0.0    |   LRT=1  Cross= 1      |

Looking at these values we can be quite satisfied, the parameters don't substantially influence the results and what seems to be more discriminating is the value of n (and that of p would also be).


In the second case we consider an hub, in which we have a central node to which all the others are connected;we consider a D matrix in which there are 30 edges, so we are not testing a single-link. 
To obtain the size we procedeed as before, with an A compatible with H0, while for the power we considered several alternative hypotheses (as in the paper). 
Since the values in A represent the strength of the edge, we first considered very weak and gradually stronger bonds. In this way we have data that are not compatible with H0 but very similar or completely different.
```{r, eval= F}
p=25 #always
### H0: F = { (p,2) }, and A[F] = 0
D <- matrix(0, p, p)
D[2:16,c(p-1,p)] = 1

D
### Adjacency Matrix >> Hub
# All connected to 1, NO EDGE between p-2 >> **COMPATIBLE** with H0
A      <- matrix(0, p, p)     
A[, 1] <- sign( runif( p, min = -1, max = 1 ) )
A[1,1] <- 0
# **COMPATIBLE** with H1 (power) edges in (p,2):
#A[2:16,c(p-1,p)] =0.1 nearest
#A[2:16,c(p-1,p)] =0.3
#A[2:16,c(p-1,p)] =0.5
#A[2:16,c(p-1,p)] =1 the most distant
Unn <- rep(NA, M)
Wnn <- rep(NA, M)
for(m in 1:M) {
  X   <- matrix( rnorm(n*p), n, p) %*% t(solve(diag(p) - A) )
  # split the data
  split = split_data(X)
  tr = split$train; te = split$test
  
  Unn[m] = linkage_test(X, D, idx, alpha,mu)$lrt
  Wnn[m]=linkage_test(X, D, idx, alpha,mu)$crossfit
  progress(m, M)
}

cat('\nLRT size: ',sum(Unn)/M , '\n')
cat('Crossfit size: ',sum(Wnn)/M, '\n')
#cat('\nLRT power: ',sum(Unn)/M , '\n')
#cat('Crossfit power: ',sum(Wnn)/M, '\n')

```
|     Parameters            | SIZE                     |                 
|:-------------------------:|:------------------------:|
| M=1000, n=200,alpha=0.05  | LRT= 0.097 Cross= 0.015  | 
| M=2000, n=200, alpha=0.05 | LRT= 0.1205 Cross= 0.195 | 
| M=1000, n=500, alpha=0.05 | LRT= 0 Cross= 0          |


So they are slightly higher than the other example  for "small" $n$ and as it increase the seems working very well. Regarding the power:

| Parameters                | Choice of H1           | POWER                  |
|:-------------------------:|:----------------------:|:----------------------:|
| M=1000, n=200,alpha=0.05  | A[p,2]=0.1             | LRT=0.913 Cross= 0.827 |
| M=1000, n=200, alpha=0.05 | A[p,2]=0.3             | LRT=0.891 Cross= 0.796 |
| M=1000,n=200, alpha=0.05  | A[p,2]=0.5             | LRT= 0.845 Cross=0.696 |
| M=1000,n=200, alpha=0.05  | A[p,2]=1               | LRT=1 Cross=1          |

As we would have expected, the test works best when A differs more from the null hypothesis (or better from the adjacency matrix compatible with H0)that is, when $A[p, 2] = 1$ because in this case it is easier to discriminate between the two.

With this second example the results are less good, but since it is a real test procedure what we have to aim for is a trade-off between the values of $ \ alpha $ and $ \ beta $ (and therefore the power) and therefore the values we get are more than acceptable.

**Directed pathway**


A directed pathway is specified by an index set $F$ of size $|F|$ where a common segment is shared by any two consecutive indices, like $F = {(j1, j2), (j2, j3), ..)}$.
We are interested in testing:
$H0$ : $A[j, k] = 0$ for some $(j, k) ∈ F$ vs $H1$ : $A[j, k]  = 0$  $\forall (j, k) ∈ F$

In this case, the $ U_n $ statistic has a different form, so we created two more functions that calculate the numerator and denominator, respectively.
```{r, eval= FALSE}
lik_est_mod <- function(A, X, idx, i){
  
  # this is a modified version of the logLikelihood that computes the logL 
  # on a modified adjacency matrix where we put a zero for one of the edges
  # belonging to the path. Vectorizing the function we can pass directly 
  # the vector of cells of the path instead of a single index
  
  A[i] = 0 # modifying the adjacency matrix
  sig = sigma_est(A, X, idx)
  c = 1/(2*sig)
  a = (dim(X)[1]/2)*log(sig)
  s = 0
  for(j in 1:length(idx)) s = s + c*sigma_square(A, X, j, idx) + a
  return(-s)
  
}

lik_est_mod = Vectorize(lik_est_mod, vectorize.args = 'i') # vectorizing over the index

find_denominator <- function(A, D, X, idx){
  
  # this function finds the denominator of the Un test statistc
  
  path = which(D != 0)
  return(max(lik_est_mod(A, X, idx, path))) # find the max along all the logL
  
}
```
So this is our "new" Constrained Likelihood Ratio Statistics.

```{r, eval= FALSE}
Un_path <- function(tr, te, idx, mu, D){
  
  # computes the test statistic for the pathway test
  
  out <- MLEdag(X=tr,D=D,tau=0.35,mu=mu,rho=1.2,trace_obj=FALSE)
  A.0 = out$A.H0; A = out$A.H1
  sig_alt = sigma_est(A, te, idx)
  denominator = find_denominator(A.0, D, tr, idx)
  x = lik_est(A, tr, idx, sig_alt) - denominator
  
  return(x)
  
}

```
To test size and power in the case of the pathway (which perhaps was not required but by now we had done it) we randomly generated both the initial path (the "true" one) and the hypotheses.

```{r, eval=FALSE}
#Initial path
generate_path <- function(p, len){
  
  # this function generates a matrix D with a random path of a given length
  
  D = matrix(0, p, p)
  v <- sample(c(1:(p-len)), 1)
  for(i in v:(v+len-1)) D[i, i+1] = 1
  return(D)
  
}
#Data compatible with H0 for size
generate_hypo_path <- function(p, len){
  
  # this function generates a null hypothesis for the path test
  # by setting in A to 0 every cell that is a nonzero in D
  
  A <- generate_matrix(p)
  D <- generate_path(p, len)
  path <- which(D != 0) # find the path cells
  
  k <- sample(c(1:(len-1)), 1) # select a random number of edges < path length
  idx <- sample(path, k) # pick k random cells to equal to zero
  A[idx] = 0
  
  return(list(D=D, A=A))
  
}
#Data compatible with H1 for power
generate_alt_hypo_path <- function(p, len){
  
  # this function generates a null hypothesis for the linkage test
  # by setting in A to 0 every cell that is a nonzero in D
  
  A <- generate_matrix(p)
  D <- generate_path(p, len)
  path <- which(D != 0) # find the path cells
  A[path] = runif(len, -1, 1)
  
  return(list(D=D, A=A))
  
}

```
Finally this is the real simulation, in which we have also inserted $W_n$ obtained simply by exchanging the roles of train and test and dividing by two. Also since this time we have randomly generated the hypotheses, for convenience we use two different functions for size and power.

```{r, eval=FALSE}
pathway_test <- function(X, D, idx, alpha, mu){
  
  # this function implements a universal linkage test
  
  # split the data
  split = split_data(X)
  tr = split$train; te = split$test
  
  # compute the test statistics Un and Wn
  Un_base = Un_path(tr, te, idx, mu, D)
  Un_swap = Un_path(te, tr, idx, mu, D)
  Wn = (exp(Un_base) + exp(Un_swap))/2
  
  # test decision
  lik_ratio = Un_base > -log(alpha)
  lik_ratio_swap = Wn > 1/alpha
  
  return(list(lrt=lik_ratio, crossfit=lik_ratio_swap))
  
}

compute_size_path <- function(M, p, n, alpha, len, mu){
  
  # this function computes the size of a linkage test
  # running M tests and computing the size as the number of rejections
  # over the number of simulations under the null hypothesis
  
  idx = c(1:p)
  hypo = generate_hypo_path(p, n, len)
  D = hypo$D; A = hypo$A
  
  size_lrt = rep(NA, M)
  size_crossfit = rep(NA, M)
  
  for(m in 1:M){
    X = generate_data(A, n, p)
    exit = pathway_test(X, D, idx, alpha, mu)
    size_lrt[m] = exit$lrt
    size_crossfit[m] = exit$crossfit
    progress(m, M)
  }
  
  cat('LRT size: ', sum(size_lrt)/M, '\n')
  cat('Crossfit size: ', sum(size_crossfit)/M)
}

compute_power_path <- function(M, p, n, alpha, len, mu){
  
  # this function computes the power of a linkage test running M tests
  # and computing the power as one minus the number of acceptances
  # over the number of simulations under the alternative hypothesis
  
  idx = c(1:p)
  hypo = generate_alt_hypo_path(p, n, len)
  D = hypo$D; A = hypo$A
  
  power_lrt = rep(NA, M)
  power_crossfit = rep(NA, M)
  
  for(m in 1:M){
    X = generate_data(A, n, p)
    exit = pathway_test(X, D, idx, alpha, mu)
    power_lrt[m] = exit$lrt
    power_crossfit[m] = exit$crossfit
    progress(m, M)
  }
  
  cat('LRT power: ', 1 - (M-sum(power_lrt))/M, '\n')
  cat('Crossfit power: ', 1 - (M-sum(power_crossfit))/M)
}


```

The results obtained using len = 3 (path length) and p=15 are:


| Parameters                | SIZE                   | POWER                  |
|:-------------------------:|:----------------------:|:----------------------:|
| M=200, n=500,alpha=0.05   | LRT= 0.2 Cross= 0.3    | LRT= 0.96 Cross= 1     |
| M=1000, n=1000, alpha=0.05| LRT= 0.05 Cross= 0.117 | LRT=0.997 Cross= 1     |
| M=2000,n=1500, alpha=0.05 | LRT= 0 Cross= 0        | LRT=1 Cross= 1         |


# Cytometry Data
## 4. Formalized hypotheses 
**Linkage-type **

1. **PIP3 → Akt**

\begin{align*}
H0= A[PIP3,Akt]= 0  \: \:;  H1= not\; H0
\end{align*}


It's categorized as a missing edge but it’s a well-established influence connection in literature, so it might seem strange that is not present. 
Actually there are different explanation for this; first of all the fact that Bayesian networks are constrained to be acyclic, so if the underlying network contains feedbackloops, we cannot necessarily expect to cover all connections.
We also have a limited sample available, with few proteins in specific sites, so it may happen that PIP3 is phosphorylated in a site other than that received by the cytophlorimeter or there is an intermediate (which means "both Akt and PIP3 activated but I cannot prove that it was PIP3 directly").

2. **Erk → Akt** 

\begin{align*}
H0= A[Erk,Akt]= 0  \: \:;    H1= not\; H0
\end{align*}

3. **PKC → PKA**

\begin{align*}
H0= A[PKC,PKA]= 0   \: \:;   H1= not\; H0
\end{align*}

We chose both because they are not well known in literature and are therefore important to study in order to discover things that are still unknown.
They are categorized as reported: the PKC-to-PKA connection was found once in rat ventricular myocytes and the Erk-to-Akt one in colon cancer cell .
They are also interesting because the show the ability of this Bayesian network analysis to correctly infer causal influences from unperturbed molecules within a network. In this case, Erk was not directly acted on by any activator or inhibitor in the sample sets,yet Erk showed an influence connection to Akt and the model thus predicts that direct perturbation of Erk would influence Akt.
Furthermore, another interesting aspect is that even if Erk and PKA are correlated the model predicts Erk's perturbation is not expected to affect PKA. 
This led to study the connection experimenting on the two componentsof Erk: Erk1, Erk2 and to discover that Akt phosphorylation was not affected by the inhibition of Erk2. The connection between Erk1 and Akt can be direct or indirect, involving the mediator molecules yet to be understood, but the connection is supported by both the model and the validation experiment.

4. **PKC → pjnk**

\begin{align*}
H0= A[PKC,pjnk]= 0    \: \:;  H1= not\; H0
\end{align*}


We decided to test this edge as well to have an "Expected" type.

**Pathway-type**

**PKC→ Raf→ Mek → Erk → Akt**

$F=$ (PKC,Raf),(Raf,Mek),(Mek, Erk),(Erk,Akt) 

$H0$ : $A[j, k] = 0$ for some $(j, k) ∈ F$ vs  \; $H1$ : $A[j, k]  = 0$  $\forall (j, k) ∈ F$

Actually for testing purpose we consider the presence of the entire path ($A[j, k] = 0$ for all $(j, k) \in F$)

We choose this path for two reason.The first is that there are two links from PKC to Mek but none are redundant.
Infact  the influenceof PKC on Mek, is known to be mediated by Raf through two paths of influence, each mediated by a different active phosphorylated form of the protein Raf. Since in the experiment it was not carried out Raf phosphorylated at S259, one of the two is a direct link but it is not in reality.
The second reason is that Akt  is a protein that is frequently found mutated in tumors, and it is one of the current problems of research in this area to understand "at what point" this mutation occurs in the signaling pathway. For example try to understand if it is further downstream (therefore due to the activation of Mek or Erk) or is it a "primary" mutation (eg PKC).

## 5. Data normalization and Hypotheses testing  with a single intervention

To start we choose the last intervention (the 9-th) and check that the data fits the model: they need be zero-mean Gaussian.
![](C:\Users\Umberto\Downloads\original.png)


As you can see in the plots above, this is not verified, so we first scaled them and then we applied the boxcox transformation (the unit variance was not necessary but will be useful later when merging heterogeneous data).
For the boxcox transformation, we have estimated $\lambda^*$ to use with a linear regression that considers only the intercept.
This is the final result, that we can consider as zero-mean Gaussian:


![](C:\Users\Umberto\Downloads\Rplot.png)

With this data we "double-check" the previous hypothesis, either  with our $U_n$ and $W_n$ either with the asymptotic procedure of MLEdag(),initially by doing M = 1000 simulations and setting the sparsity (mu) to 1 and then testing different values of the sparsity.
The results of our tests are the sums of the true and false obtained, remembering that TRUE means rejecting H0.

 **PIP3 → Akt**
 
| Sparsity       | LRT   | Crossfit  | chi-square asymp.|     
|:--------------:|:-----:|:---------:|:----------------:|
| $\mu = 0.3$    |0.315  |0.661      |False             |
| $\mu=1$        |0.336  |0.667      |False             |
| $\mu = 5$      |0.335  |0.654      |False             |    

In this case, based on the crossfit, there are more cases in which we reject H0, that however corresponds to the real structure, the other two give us the right result and a high sparsity value seems to work better but with minimal differences.


**Erk → Akt** 

| Sparsity                  | LRT                    | Crossfit               | chi-square asymp.|     
|:-------------------------:|:----------------------:|:----------------------:|:----------------:|
|    $\mu = 0.3$            |            0.724       |           1            |True              |
|   $\mu = 1$               |  0.713                 |     1                  |True              |
| $\mu = 5$                 | 0.766                  |         1              |True              |    

All three tests always detect the presence of the edge (as in the paper) and therefore give the correct result; sparsity doesn't seem to be very influential.

**PKC → PKA**

| Sparsity                  | LRT                    | Crossfit               | chi-square asymp.|     
|:-------------------------:|:----------------------:|:----------------------:|:----------------:|
|    $\mu = 0.3$            |            0.318       |           0.638        |False             |
|   $\mu = 1$               |  0.34                  |     0.678              |False             |
| $\mu = 5$                 | 0.323                  |         0.659          |False             | 

With this connection we got the worst results: the ideal result would be "1, 1, True". The chi-square and LRT do not really detect the presence of the arch, while the crossfit gives us the correct solution but differs little from the random case (0.5).

**PKC → pjnk**

| Sparsity                  | LRT                    | Crossfit               | chi-square asymp.|     
|:-------------------------:|:----------------------:|:----------------------:|:----------------:|
|    $\mu = 0.3$            |            0.503       |           0.881        |True              |
|   $\mu = 1$               |  0.476                 |     0.887              |True              |
| $\mu = 5$                 | 0.5                    |         0.893          |True              |    

Once again crossfit achieves good results, such as chi-square, while LRT leaves us in a situation of uncertainty, in which we would not be able to make a decision.

**PKC→ Raf→ Mek → Erk → Akt**

| Sparsity                  | LRT                    | Crossfit               | chi-square asymp.|     
|:-------------------------:|:----------------------:|:----------------------:|:----------------:|
|    $\mu = 0.3$            |            0.753       |               1        |True              |
|   $\mu = 1$               |  0.793                 |     1                  |True              |
| $\mu = 5$                 | 0.753                  |         0.999          |True              |  

In the case of the path, all three statistics return the correct result, effectively confirming the presence of it.

Since it was required to vary the sparsity  (that is a and compare the results, not having obtained any particular "trend", we tried to test Erk_Akt with M = 200 by trying different values of mu (especially in the 0-1 interval after understanding that mu can be considered as the "weight" of the constraint in our problem).

![](C:\Users\Umberto\Desktop\Rplot05.png)

In the first graph there are only the values zoomed up to 1, on the right also the higher values. Despite this "inside" the values  are all very close to each other and there is no particular trend based on the value of the sparsity (or better mu).


## 6. Repeating with more data

Since we have to consider the data of 9 different interventions / perturbations, the adjustment for multiplicity is needed. The probability of obtaining at least one positive test (p <0.05) by chance, and therefore an error, is:

\begin{align*}
P = 1-(1-\alpha)^{k}
\end{align*}

where k=9 is the number of tests conducted. With $\alpha=0.05$ it would be about $37%$.We used Bonferroni correction to control the Family-Wise Error Rate, as we believe that when studying the human immune system (and therefore diseases and related treatments) we don't want "(any) bad surprise ".

(While the correction by Benjamini-Hochberg which controls the False discovery rate, that is the expected percentage of false positives among the tests that turn out to be so, we believe it's "relatively less important" in a situation like this...).

We also chose Bonferroni because we believe that a study like the one we are dealing with needs caution in decreeing a statistically significant test and this is a very conservative method but also because we have rather heterogeneous data. Surely the 9 interventions will be related in some way, but the variety of biological effects of the perturbations employed (Table 1 of Sachs et al. (2005)) allows us to say that we do not run the risk of actually having a single test.

The "new" Decision Rule is: 
Reject $H0$ if $p < \dfrac {\alpha}{k}$ where k= number of tests=9 and we have to consider the p-value for each of them.

We know that with Bonferroni the cost of the protection against type I errors is an increased risk of failing to reject one or more false null hypotheses (i.e., of committing one or more type II errors).

The Holm - Bonferroni method also controls the maximum family-wise error rate at level  $\alpha$, but with a lower increase of type II error risk.
We have to consider the $k=9$ p-value sorted into order lowest-to-highest and their corresponding hypotheses.
For each p-value, test whether 
\begin{align*}
p < {\frac {\alpha }{m+1-k}}
\end{align*}
If so, reject H0 for that specific test and continue to examine the larger p-values, otherwise exit.

To actually evaluate how conservative the correction is against H0, we checked the assumptions in both ways. In all cases we have set $\mu =1$.

**PIP3 → Akt**
 
| Bonf. correction  | LRT   | Crossfit  | chi-square asymp.|     
|:-----------------:|:-----:|:---------:|:----------------:|
| Yes               |0.002  |0.004      |False             |
| No                |       |           |                  |


Using all 9 datasets and with the correction, we obtain excellent results (remember that
here the edge is actually absent).


**Erk → Akt** 

 
| Bonf. correction  | LRT   | Crossfit  | chi-square asymp.|     
|:-----------------:|:-----:|:---------:|:----------------:|
| Yes               |0.52   |1          |False             |
| No                |       |           |                  |

In this case, with the correction, only crossfit works well; we believe that at least in part it is due to the fact that Bonferroni tends to preserve H0.

**PKC → PKA**

| Bonf. correction  | LRT   | Crossfit  | chi-square asymp.|     
|:-----------------:|:-----:|:---------:|:----------------:|
| Yes               |       |           |                  |
| No                |       |           |                  |

**PKC → pjnk**

| Bonf. correction  | LRT   | Crossfit  | chi-square asymp.|     
|:-----------------:|:-----:|:---------:|:----------------:|
| Yes               |       |           |                  |
| No                |       |           |                  |


**PKC→ Raf→ Mek → Erk → Akt**

| Bonf. correction  | LRT   | Crossfit  | chi-square asymp.|     
|:-----------------:|:-----:|:---------:|:----------------:|
| Yes               |       |           |                  |
| No                |       |           |                  |


